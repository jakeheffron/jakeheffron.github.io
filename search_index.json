[
["introduction.html", "TCU BIS Data Science Competition - Sneakers 1 Introduction", " TCU BIS Data Science Competition - Sneakers 1 Introduction Authors: Jake Heffron, Jake Hennes, Kailey Hetler, Truett Pittman Last Updated: 08/06/2020 This summer, our team has been working on various sentiment-based visualizations and a multi-linear regression model to try to predict the prices of highly sought-after sneakers. Jake Heffron, Jake Hennes, Kailey Hetler, and Truett Pittman, have utilized several R packages to analyze Tweets leading up to a new sneaker’s release to gain understanding of the public’s anticipation for the launch. This website explains how the team gathered, compiled, and analyzed the data to support the team’s hypothesis. In theory, the relationship between tweet sentiment and the resell markup from original retail price will allow the team to predict a sneaker’s selling price before its launch. The resale price was found using StockX, which is a platform that provides various information about the latest sneakers, along with a channel to purchase them. Resale prices are stored and can be viewed over time on StockX’s website. Initially, the team set out to gather thousands of tweets to analyze. Using the rtweet package and a custom twitter developer app, the team pulled tweets from the days leading up to seven individual sneaker launches. The following shoes were analyzed: Nike Air Max Green Nike Air Max Orange Grateful Dead Dunk Jordan University Gold Off White Jordan Sail Yeezy Blue Oat Jordan XI Low Concord "],
["extracting-twitter-data.html", "2 Extracting Twitter Data", " 2 Extracting Twitter Data For each sneaker, a data frame was created showing various fields relating to the tweets, such as the tweets’ content, date, retweet count, etc. The code utilizes a Twitter developer app created by the team to query tweets that contain the desired search strings. The team used the names of the sneakers as the search strings for each query. Since no team member had a paid Twitter developer count, Twitter limited the query to tweets within the past seven days. However, the team noticed more activity in the 4 to 5 days leading up to a sneaker’s release than any time beforehand, therefore, the team wasn’t overly concerned about collecting only 7 days worth of data. The following code shows how the team utilized the rtweet package to query the results. library(httpuv) library(rtweet) twitter_token &lt;- create_token( app = &quot;frog_sneaker_data&quot;, consumer_key = &quot;consumer_key&quot;, consumer_secret = &quot;consumer_secret&quot;, access_token = &quot;access_token&quot;, access_secret = &quot;access_secret&quot;) tweet_search_string &lt;- &quot;Jordan 11 Low Concord&quot; Kmax_number_of_tweets = 1000 sneaker_tweets &lt;- search_tweets( q = tweet_search_string, n = Kmax_number_of_tweets, include_rts = FALSE, lang = &quot;en&quot;, geocode = lookup_coords(&quot;usa&quot;), retryonratelimit = FALSE) "],
["combining-data.html", "3 Combining Data", " 3 Combining Data Given that the rtweet data frames contained over 90 attributes for each query, the team condensed the data frames down to the individual tweets. Next, utilizing the tidytext and dplyr packages, individual words from tweets were extracted and tokenized into another data frame to then evaluate sentiment. Stop words such as “and,” “but,” and “the,” among others, were also extracted since they aren’t as valuable for evaluating sentiment as other descriptive words in the tweets. The team utilized the basic tidytext data set containing the stop words. library(tidytext) library(dplyr) jordan_11_low_concord_tweets_df &lt;- tibble(created_at = jordan.11.low.concord.tweets$created_at, text = jordan.11.low.concord.tweets$text) tidy_jordan_11_low_concord &lt;- jordan_11_low_concord_tweets_df %&gt;% unnest_tokens(word, text) tidy_jordan_11_low_concord &lt;- tidy_jordan_11_low_concord %&gt;% anti_join(stop_words, by = c(&quot;word&quot; = &quot;word&quot;)) The data frame was further modified using the lubridate package to generate a more meaningful way to classify dates from individual words within the tweets. library(lubridate) # for the function as_date tidy_jordan_11_low_concord$created_at &lt;- as_date(tidy_jordan_11_low_concord$created_at) # By date, drop time stamp so we can generate a meaningful count by time jordan_11_low_tweet_word_count_by_date &lt;- tidy_jordan_11_low_concord %&gt;% group_by(created_at) %&gt;% count(word, sort = TRUE) ungroup(jordan_11_low_tweet_word_count_by_date) Then, to evaluate the sentiment of individual words, the team created a list of common slang words related to “sneaker heads,” or those engaged in the frequent buying and selling of the hottest sneakers, and assigned values ranging from (-5,5) with -5 being the most negative and 5 being the most positive. The slang words were then combined with the afinn dataset containing a list of about 2400 other descriptive words in the english dictionary. However, because afinn already had duplicate words contained in the team’s “slang dictionary,” duplicate words were dropped from the afinn dataset so that the values generated by the team were utilized for the duplicate words. Please note that common slang words may be rather inapproriate, however the intent was to properly evaluate the sentiment of modern culture towards popular sneakers. Finally, a new data frame called “j_11_low_sentiment_data_by_date” was created to store the number of words analyzed per day, the sum of sentiment per day, and the number of negative words per day for the “Jordan 11 Low Concord.” This process was repeated for each sneaker. library(textdata) # Supports afinn, (-5 to 5), bing, loughran, and nrc lexicons jordan_11_low_concord_afinn &lt;- tibble(word = c(&#39;sick&#39;, &#39;rad&#39;, &#39;dope&#39;, &#39;badass&#39;, &#39;hype&#39;, &#39;chill&#39;, &#39;fly&#39;, &#39;fire&#39;, &#39;lit&#39;, &#39;fleek&#39;, &#39;goals&#39;,&#39;need&#39;, &#39;hit&#39;, &#39;L&#39;, &#39;trash&#39;, &#39;garbage&#39;, &#39;legit&#39;, &#39;hook&#39;,&#39;cop&#39;,&#39;buy&#39;,&#39;finna&#39;,&#39;purchase&#39;,&#39;sleezy&#39;,&#39;win&#39;,&#39;lose&#39;,&#39;hoed&#39;, &#39;suck&#39;,&#39;bitch&#39;,&#39;shit&#39;,&#39;damn&#39;,&#39;fuck&#39;,&#39;ass&#39;,&#39;cheeks&#39;,&#39;gay&#39;,&#39;lowkey&#39;,&#39;highkey&#39;,&#39;steal&#39;, &#39;bad&#39;, &#39;boujee&#39;, &#39;bought&#39;,&#39;buying&#39;,&#39;bread&#39;, &#39;called&#39;, &#39;dreamy&#39;, &#39;ima&#39;,&#39;imma&#39;,&#39;nah&#39;,&#39;pass&#39;,&#39;gtfo&#39;, &#39;lol&#39;,&#39;pricey&#39;, &#39;expensive&#39;,&#39;smoke&#39;,&#39;wet&#39;,&#39;snipe&#39;,&#39;stylish&#39;,&#39;style&#39;, &#39;fresh&#39;,&#39;grateful&#39;,&#39;dead&#39;,&quot;abomination&quot;,&#39;heat&#39;,&#39;hot&#39;,&#39;hottest&#39;,&#39;ice&#39;,&#39;lax&#39;,&#39;amped&#39;,&#39;stoked&#39;,&#39;bomb&#39;,&#39;grease&#39;,&#39;bro&#39;,&#39;bruh&#39;,&#39;juice&#39;, &#39;peak&#39;,&#39;fucking&#39;,&#39;gnarly&#39;,&#39;sketch&#39;,&#39;sketchy&#39;,&#39;lock&#39;,&#39;bust&#39;,&#39;steeze&#39;,&#39;steezy&#39;,&#39;yikes&#39;,&#39;extra&#39;,&#39;snatched&#39;,&#39;fit&#39;,&#39;bet&#39;,&#39;shade&#39;,&#39;flex&#39;, &#39;flexed&#39;,&#39;lewk&#39;,&#39;slay&#39;,&#39;shook&#39;,&#39;stan&#39;,&#39;yeet&#39;,&#39;sus&#39;,&#39;goat&#39;,&#39;poppin&#39;,&#39;pop&#39;,&#39;cringe&#39;,&#39;cringey&#39;,&#39;oof&#39;,&#39;mood&#39;,&#39;mewd&#39;,&#39;horny&#39;,&#39;drip&#39;,&#39;ratchet&#39;, &#39;slaps&#39;,&#39;slap&#39;,&#39;basic&#39;,&#39;yas&#39;,&#39;gag&#39;,&#39;gucci&#39;,&#39;af&#39;,&#39;turnt&#39;), value = c(3,2,4,5,3,2,2,5,4,2,4,3,2,-3,-4,-5,3,1,4,3,1,3,2,3,-3,-3,-3,-4,-5,-2,-2,-5,-4,-5,1,2,2,1,4,3,3,2,1,4,1,1,-3,-4,-5,-1,-3,-3,3,2,2,3,3,3,0,0,-5,3,3,4,3,2,3,3,4,-4,1,1,4,2,0,4,-3,-3,3,-3,3,4,-3,2,3,2,3,-2,3,3,2,3,-3,3,3,-3,4,3,3,-4,-4,-2,3,3,3,4,-4,3,3,-2,2,-4,4,2,3)) remove_word_list &lt;- tibble(word = c(&quot;bad&quot;,&quot;drop&quot;,&#39;grey&#39;, &#39;lol&#39;,&#39;mad&#39;,&#39;sick&#39;,&#39;badass&#39;, &#39;steal&#39;,&#39;suck&#39;,&#39;fire&#39;,&#39;ass&#39;,&#39;shit&#39;,&#39;bitch&#39;,&#39;win&#39;, &#39;grateful&#39;, &#39;dead&#39;),value = c(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0)) afinn &lt;- anti_join(get_sentiments(&quot;afinn&quot;),remove_word_list, by = &quot;word&quot;) afinn_custom &lt;- rbind(jordan_11_low_concord_afinn, afinn) j_11_low &lt;- inner_join(jordan_11_low_tweet_word_count_by_date, afinn_custom) # add the afinn variable of value j_11_low_sentiment_data_by_date &lt;- j_11_low %&gt;% group_by(created_at) %&gt;% summarise(n = n(), # social media count total_sentiment = sum(value), # sum of sentiment negative_words = length(value[value &lt; 0]), # occurrences of negative words ) Below is the end result of the data frame for the Jordan 11 Low Concords: X created_at n total_sentiment negative_words 1 2020-07-02 1 -3 1 2 2020-07-03 7 8 1 3 2020-07-05 5 6 2 4 2020-07-06 2 -1 1 5 2020-07-07 2 -6 2 6 2020-07-08 2 -2 1 7 2020-07-09 5 12 1 8 2020-07-10 8 6 3 "],
["data-visualization.html", "4 Data Visualization 4.1 Preparing Data 4.2 Histogram 4.3 Line Chart 4.4 Scatter Plots", " 4 Data Visualization 4.1 Preparing Data Additional data preparation was needed to create the visualizations shown later in this section. An additional column was added to the each sneaker’s sentiment data frame. This column depicts the name of the sneaker, making it easier to group each sneaker’s data in the visualizations after the data frames were combined to form the final data frame used for the histogram and line chart visualizations, called “final_data_frame.” Also, a new column was added to the final_data_frame counting down the days from each sneakers’ launch. j_11_low_sentiment_data_by_date$source &lt;- c(&quot;Jordan 11 Low Concord&quot;, &quot;Jordan 11 Low Concord&quot;, &quot;Jordan 11 Low Concord&quot;, &quot;Jordan 11 Low Concord&quot;, &quot;Jordan 11 Low Concord&quot;, &quot;Jordan 11 Low Concord&quot;, &quot;Jordan 11 Low Concord&quot;, &quot;Jordan 11 Low Concord&quot;) final_data_frame &lt;- rbind(final_data_frame, j_11_low_sentiment_data_by_date) final_data_frame$day &lt;- c(7,6,5,4,3,2,1,0,8,7,6,5,4,3,2,1,0,8,7,6,5,4,3,2,1,0,8,7,6,5,4,3,2,1,0,7,6,5,4,3,2,1,0,8,7,6,5,4,3,2,1,0,8,7,6,5,4,3,2,1,0) Below is the final data frame for all of the seven shoes: X created_at n total_sentiment negative_words source day 1 2020-07-02 1 -3 1 Jordan 11 Low Concord 7 2 2020-07-03 7 8 1 Jordan 11 Low Concord 6 3 2020-07-05 5 6 2 Jordan 11 Low Concord 5 4 2020-07-06 2 -1 1 Jordan 11 Low Concord 4 5 2020-07-07 2 -6 2 Jordan 11 Low Concord 3 6 2020-07-08 2 -2 1 Jordan 11 Low Concord 2 7 2020-07-09 5 12 1 Jordan 11 Low Concord 1 8 2020-07-10 8 6 3 Jordan 11 Low Concord 0 9 2020-07-22 1 -2 1 Air Max Orange 8 10 2020-07-23 3 9 0 Air Max Orange 7 11 2020-07-24 4 3 2 Air Max Orange 6 12 2020-07-25 2 4 0 Air Max Orange 5 13 2020-07-26 4 14 0 Air Max Orange 4 14 2020-07-27 4 11 1 Air Max Orange 3 15 2020-07-28 6 15 0 Air Max Orange 2 16 2020-07-29 4 4 1 Air Max Orange 1 17 2020-07-30 7 0 4 Air Max Orange 0 18 2020-07-22 4 5 1 Air Max Green 8 19 2020-07-23 2 6 0 Air Max Green 7 20 2020-07-24 5 9 1 Air Max Green 6 21 2020-07-25 2 2 1 Air Max Green 5 22 2020-07-26 2 4 0 Air Max Green 4 23 2020-07-27 2 4 1 Air Max Green 3 24 2020-07-28 1 5 0 Air Max Green 2 25 2020-07-29 3 2 1 Air Max Green 1 26 2020-07-30 7 7 2 Air Max Green 0 27 2020-07-16 2 0 0 Grateful Dead Dunk 8 28 2020-07-17 9 7 2 Grateful Dead Dunk 7 29 2020-07-18 9 5 1 Grateful Dead Dunk 6 30 2020-07-19 6 12 0 Grateful Dead Dunk 5 31 2020-07-20 14 27 0 Grateful Dead Dunk 4 32 2020-07-21 25 33 5 Grateful Dead Dunk 3 33 2020-07-22 18 25 2 Grateful Dead Dunk 2 34 2020-07-23 29 35 6 Grateful Dead Dunk 1 35 2020-07-24 34 25 11 Grateful Dead Dunk 0 36 2020-07-17 6 9 1 Jordan 12 University Gold 7 37 2020-07-18 3 5 1 Jordan 12 University Gold 6 38 2020-07-19 8 4 3 Jordan 12 University Gold 5 39 2020-07-20 7 18 0 Jordan 12 University Gold 4 40 2020-07-21 17 23 3 Jordan 12 University Gold 3 41 2020-07-22 9 15 2 Jordan 12 University Gold 2 42 2020-07-23 14 19 3 Jordan 12 University Gold 1 43 2020-07-24 32 22 10 Jordan 12 University Gold 0 44 2020-07-17 1 1 0 Yeezy 380 Blue Oat 8 45 2020-07-18 2 5 0 Yeezy 380 Blue Oat 7 46 2020-07-19 3 4 1 Yeezy 380 Blue Oat 6 47 2020-07-20 9 4 3 Yeezy 380 Blue Oat 5 48 2020-07-21 9 13 1 Yeezy 380 Blue Oat 4 49 2020-07-22 4 11 0 Yeezy 380 Blue Oat 3 50 2020-07-23 16 16 3 Yeezy 380 Blue Oat 2 51 2020-07-24 14 16 3 Yeezy 380 Blue Oat 1 52 2020-07-25 20 42 2 Yeezy 380 Blue Oat 0 53 2020-07-17 5 13 0 Off-White Jordan 4 Sail 8 54 2020-07-18 2 0 0 Off-White Jordan 4 Sail 7 55 2020-07-19 2 6 0 Off-White Jordan 4 Sail 6 56 2020-07-20 11 12 2 Off-White Jordan 4 Sail 5 57 2020-07-21 11 13 3 Off-White Jordan 4 Sail 4 58 2020-07-22 3 10 0 Off-White Jordan 4 Sail 3 59 2020-07-23 9 12 3 Off-White Jordan 4 Sail 2 60 2020-07-24 9 3 4 Off-White Jordan 4 Sail 1 61 2020-07-25 22 -13 13 Off-White Jordan 4 Sail 0 A second data frame, called “scatter_frame,” was also created to be used for the scatter plot visualization. Essentially, the sentiment totals per day in the “final_data_frame” were summed to create a total sentiment value per sneaker spanning across the time frame in which the tweets were collected. This summation was then added to the “scatter_frame.” Moreover, StockX data, including each sneaker’s retail price, average resale price, and number of sales on the StockX platform, were manually collected from their website, and then added to the “scatter_frame.” Additionally, another column, called the “markup,” was calculated by dividing the average resale price by the retail price. #Store Total Sentiment for each sneaker in scatter_frame j_ll_sent_sum &lt;- final_data_frame %&gt;% filter(source == &quot;Jordan 11 Low Concord&quot;) %&gt;% summarise(sum(total_sentiment)) ugdd_sent_sum &lt;- final_data_frame %&gt;% filter(source == &quot;Grateful Dead Dunk&quot;) %&gt;% summarise(sum(total_sentiment)) jug_sent_sum &lt;- final_data_frame %&gt;% filter(source == &quot;Jordan 12 University Gold&quot;) %&gt;% summarise(sum(total_sentiment)) amo_sent_sum &lt;- final_data_frame %&gt;% filter(source == &quot;Air Max Orange&quot;) %&gt;% summarise(sum(total_sentiment)) amg_sent_sum &lt;- final_data_frame %&gt;% filter(source == &quot;Air Max Green&quot;) %&gt;% summarise(sum(total_sentiment)) ybo_sent_sum &lt;- final_data_frame %&gt;% filter(source == &quot;Yeezy 380 Blue Oat&quot;) %&gt;% summarise(sum(total_sentiment)) owjs_sent_sum &lt;- final_data_frame %&gt;% filter(source == &quot;Off-White Jordan 4 Sail&quot;) %&gt;% summarise(sum(total_sentiment)) #Combine Rows to scatter_frame scatter_frame &lt;- rbind(j_ll_sent_sum, ugdd_sent_sum) scatter_frame &lt;- rbind(scatter_frame, jug_sent_sum) scatter_frame &lt;- rbind(scatter_frame, amo_sent_sum) scatter_frame &lt;- rbind(scatter_frame, amg_sent_sum) scatter_frame &lt;- rbind(scatter_frame, ybo_sent_sum) scatter_frame &lt;- rbind(scatter_frame, owjs_sent_sum) #Store Stock X data in scatter_frame scatter_frame$source &lt;- c(&quot;Jordan 11 Low Concord&quot;, &quot;Grateful Dead Dunk&quot;, &quot;Jordan 12 University Gold&quot;, &quot;Air Max Orange&quot;, &quot;Air Max Green&quot;, &quot;Yeezy 380 Blue Oat&quot;, &quot;Off-White Jordan 4 Sail&quot;) scatter_frame$retail_price &lt;- c(185, 110, 190, 140, 140, 230, 200) scatter_frame$avg_resale_price &lt;- c(223,1175, 267, 253, 263, 279, 1090) scatter_frame$sales &lt;- c(5238, 1675, 18595, 620, 728, 1039, 4744) scatter_frame &lt;- transform(scatter_frame, markup = avg_resale_price /retail_price ) Below is the final data frame utilized for the scatter plot visualizations. Reference the original retail price and resell price of each sneaker as you move through the visualizations: X sum_total_sentiment source retail_price avg_resale_price markup sales 1 20 Jordan 11 Low Concord 185 223 1.21 5238 2 169 Grateful Dead Dunk 110 1175 10.68 1675 3 125 Jordan 12 University Gold 190 267 1.41 18595 4 58 Air Max Orange 140 253 1.81 620 5 44 Air Max Green 140 263 1.88 728 6 112 Yeezy 380 Blue Oat 230 279 1.21 1039 7 56 Off-White Jordan 4 Sail 200 1090 5.45 4744 4.2 Histogram The first visualization that the team created was a Histogram for each sneaker displaying the distribution of sentiment counts. The sneakers that displayed the least popularity were those that had negative sentiment counts or sentiment counts close to 0. In the sample, the Jordan 11 Low Concord was perceived to have the lowest popularity as shown by a skewed left tail. This could be because the shoe is a low version of the Jordan 11, and low-top Jordan 11’s historically have not been as popular as other Jordan sneakers. Moreover, sneaker culture is generally dominated by males, as evidenced by the number of male sneaker releases as opposed to female sneaker releases, and the Jordan 11 Low Concord’s were a womens sneaker release. Therefore, less Twitter activity might be expected from the shoe. The Air Max sneakers also didn’t receive very high sentiment counts as compared to other shoes, which might be due to the simplicity of the sneakers, as they both were just green and orange colorways of the Air Max model. Generally speaking, sneaker collaborations tend to be more popular than regular models of sneakers due to the exclusivity of the collaboration and the unique design of the sneaker, among other variables. This phenomena turned out to be true in the sample as the Grateful Dead Dunks and Off-White Jordan 4 Sails received some of the highest sentiment counts. Currently, most Off-White collaborations have been very popular as Virgil Abloh is one of the most highly-respected designers in the industry. Also, Nike Dunks have been going through a resurgence as the most popular sneaker model in the sneaker industry at the time of this study. However, one interesting trend to note is that the Off-White Jordan 4 Sail had one very low sentiment count, the lowest of the entire sample. The team recognized this might be due to the backlash on Twitter for those that didn’t receive the sneaker on Nike’s SNKR app after the release. The data collection of each of the sneakers didn’t occur at the same time on the day that each sneaker released, therefore there might be some inconsistencies in the data collection process. Nonetheless, the team believes the data collected still holds enough quality to uncover valuable insights related to sentiment. Furthermore, the Jordan 12 University Gold sneakers were moderately popular in the sample, and the Yeezy 380 Blue Oat’s received the highest sentiment count for a single day of the entire sample. The Jordan and Kanye West brands are known the be very strong in the sneaker industry, which could explain this insight. #Historgram library(ggplot2) cbPalette &lt;- c(&quot;#006600&quot;, &quot;#FF9933&quot;,&quot;#FFFF00&quot;,&quot;#0000CC&quot;, &quot;#000000&quot;, &quot;#663300&quot;, &quot;#33CCFF&quot;, &quot;FF0000&quot;) ggplot(final_data_frame) + geom_histogram(aes(x =total_sentiment, fill = source),alpha=0.6, binwidth = 10) + scale_x_continuous(breaks = seq(-20, 40, 10)) + labs(y = &quot;Occurances&quot;, x = &quot;Sentiment Values&quot;, title = &quot;Twitter Sentiment Histogram \\n&quot;, fill = &quot;Sneakers:&quot;) + theme(plot.title = element_text(hjust = 0.5)) + theme(, strip.text.x = element_text(size = 7) ) + facet_wrap(~source) + scale_fill_manual(values=cbPalette) 4.3 Line Chart The second visualization that the team created was a Line Chart illustrating the time-series aspect of each sneakers sentiment from the collection of the data, most often eight days before the release, to the time of each sneaker’s release. Most of the individual sentiment trends are outlined in the histogram chart, however, the most interesting aspect to note in the line chart is which day the sentiment for each sneaker tends to ramp up. Looking at the chart, sentiment tends to explode at about five days before the release of a sneaker. The team hypothesized that this trend was due to the weekly cycle of sneaker releases. Generally, a handful of “popular” sneakers are released each week, and sometimes more on Holiday weekends. Therefore, “sneaker heads” might only be focused on a small subset of sneakers each week before turning their attention to the next week’s sneaker releases. As stated previously, despite not having a developer’s account, the team believes that due to this trend, seven to eight days is adequate to gauge the interest in a sneaker prior to its release. However, if the team were to continue this study, additional dates before release might be studied such as when the sneaker is first announced to be released on various sneaker news sites. #Line Chart library(ggplot2) final_data_frame$day &lt;- c(7,6,5,4,3,2,1,0,8,7,6,5,4,3,2,1,0,8,7,6,5,4,3,2,1,0,8,7,6,5,4,3,2,1,0,7,6,5,4,3,2,1,0,8,7,6,5,4,3,2,1,0,8,7,6,5,4,3,2,1,0) cbPalette &lt;- c(&quot;#006600&quot;, &quot;#FF9933&quot;,&quot;#FFFF00&quot;,&quot;#0000CC&quot;, &quot;#000000&quot;, &quot;#663300&quot;, &quot;#33CCFF&quot;, &quot;FF0000&quot;) ggplot(final_data_frame, aes(x = day, y = total_sentiment, group = source)) + geom_line(aes(color = source))+ geom_point(aes(color = source))+ scale_colour_manual(values=cbPalette)+ scale_x_reverse(breaks=seq(0,8,1)) + scale_y_continuous(breaks=seq(-15,45,5))+ labs(y = &quot;Sentiment&quot;, x = &quot;Days to Launch&quot;, title = &quot;Time-Series Twitter Sentiment\\n&quot; ) + guides(color=guide_legend(&quot;Sneaker:&quot;))+ theme(plot.title = element_text(hjust = 0.5)) 4.4 Scatter Plots The third visualization that the team created was a scatter plot explaining the linear relationship between total sentiment for each shoe from the team’s collection to the sneakers release and the markup factor from retail to resale that the sneaker sold for. The correlation between the two variables was .586 which, given the small sample size, is a reasonable solid correlation. From the beginning, the team theorized that Twitter sentiment can give an accurate prediction of a sneaker’s resale price. Therefore, if the team were to continue this study and collect more data, the team believes the correlation would improve. However, it is also important to discuss the types of shoes that would be collected, as particular models and brands are more popular than others. Hence, the team would create a list of models that would be exclusively collected to further flush out the relationship between Twitter sentiment and the markup factor from retail price. #Scatter Plots and MLRM library(ggplot2) ggplot(scatter_frame, aes(x = sum_total_sentiment, y = markup)) + geom_point()+ geom_smooth(method=lm, se = FALSE)+ labs(y = &quot;Markup Factor from Retail&quot;, x = &quot;Total Sentiment&quot;, title = &quot;Correlation&quot;, caption = &quot;Correlation = 0.586&quot;)+ theme(plot.title = element_text(hjust = 0.5))+ scale_y_continuous(breaks=seq(0,12,2))+ scale_x_continuous(breaks=seq(0,250,50))+ geom_hline(yintercept = 0) + geom_vline(xintercept = 0) The fourth visualization that the team created was a scatter plot explaining the linear relationship between sales of each sneaker on the StockX platform and the markup factor from retail to resale that the sneaker sold for. The correlation was calculated to be -0.221, which isn’t remarkable, but shows a weak negative correlation. Generally, the higher the production of an individual sneaker, the less it will sell for on the resale market due to an increasing lack of exclusivity. The team believes that collecting additional data on future sneaker releases will help to more clearly define the relationship. Moreover, most companies will not release their production numbers for individual sneakers. Therefore, due to the popularity of StockX, the team used sales on StockX as a substitute for the actual production numbers. The team would consider aggregating the sales on other websites as well, such as GOAT, to better estimate production numbers in the future. Additionally, the team will set a strict timeline after the release of the sneaker to calculate the total number of sales to keep consistency across sneakers. ggplot(scatter_frame, aes(x = sales, y = markup)) + geom_point()+ geom_smooth(method=lm, se = FALSE)+ labs(y = &quot;Markup Factor from Retail&quot;, x = &quot;Sales on StockX&quot;, title = &quot;Correlation&quot;, caption = &quot;Correlation = -0.221&quot;)+ theme(plot.title = element_text(hjust = 0.5))+ scale_y_continuous(breaks=seq(0,12,2))+ scale_x_continuous(breaks=seq(0,20000,5000))+ geom_hline(yintercept = 0) + geom_vline(xintercept = 0) Finally, the team conducted a multi-linear regression model to predict the markup factor from retail based on Twitter sentiment and sales on StockX. The team acknowledges that, to properly predict the markup factor, other variables need to be added to the model, and their exclusion can skew the model. Likewise, the team did not check for other assumptions in the model, such as the mean independence assumption, due to the small amount of data the team collected. However, for the purposes of the competition, the team wanted to understand the predictive power of the two variables collected in the study. As the model shows, none of the variables are statistically significant. But, overall the model has an adjusted r squared value of 20.37% showing that 20.37% of the total variance can be explained by the variables the team concluded. Moreover, the F-test also did not meet a 10% significance level, indicating the model does not provide a better fit than an intercept only model. Therefore, the team concludes that there many other factors worth examining to accurately predict a sneaker’s markup from retail. Although, the team believes that Twitter sentiment is still a valid variable if a new model were to be created. fit &lt;- lm(markup ~ sum_total_sentiment + sales, data=scatter_frame) # build linear regression model on full data print(fit) summary(fit) "],
["conclusion.html", "5 Conclusion 5.1 Bibliography", " 5 Conclusion The team uncovered several insights as a result of the study. The distribution of sentiments for each sneaker provided clues as to why some sneakers tend to have lower resale prices than others. Characteristics of the shoe such as the model type, whether it was a collaboration, and target audience, among other variables, tended to resonate with how well each sneaker was received on the resell market. The time-series sentiment chart showed that Twitter activity seemed to spiked at about 5 days prior to the launch of the sneaker. The team theorized this is because of the weekly cycle of sneaker releases created by the industry. The relationship between Twitter sentiment and the markup factor from retail has a moderately strong positive correlation, concluding that Twitter sentiment can be a valuable resource when trying to predict sneaker resell prices. There are many other variables besides Twitter sentiment that may serve as indicators of market resell prices, and if the team were to continue the study, additional sources could be explored that would supplement the quality of the regression model. The sneaker industry is still strong even after the impacts of COVID-19 on consumer spending. 5.1 Bibliography Hvitfeldt, Emil. 2020. Textdata: Download and Load Various Text Datasets. https://CRAN.R-project.org/package=textdata. Kearney, Michael W. 2020. Rtweet: Collecting Twitter Data. https://CRAN.R-project.org/package=rtweet. Robinson, David, and Julia Silge. 2020. Tidytext: Text Mining Using ’Dplyr’, ’Ggplot2’, and Other Tidy Tools. https://CRAN.R-project.org/package=tidytext. Wickham, Hadley, and Lionel Henry. 2020. Tidyr: Tidy Messy Data. https://CRAN.R-project.org/package=tidyr. Wickham, Hadley, Romain François, Lionel Henry, and Kirill Müller. 2020. Dplyr: A Grammar of Data Manipulation. https://CRAN.R-project.org/package=dplyr. Wickham, Hadley, Winston Chang, Lionel Henry, Thomas Lin Pedersen, Kohske Takahashi, Claus Wilke, Kara Woo, Hiroaki Yutani, and Dewey Dunnington. 2020. Ggplot2: Create Elegant Data Visualisations Using the Grammar of Graphics. https://CRAN.R-project.org/package=ggplot2. Xie, Yihui. 2020b. Knitr: A General-Purpose Package for Dynamic Report Generation in R. https://CRAN.R-project.org/package=knitr. Spinu, Vitalie, Garrett Grolemund, and Hadley Wickham. 2020. Lubridate: Make Dealing with Dates a Little Easier. https://CRAN.R-project.org/package=lubridate. "]
]
